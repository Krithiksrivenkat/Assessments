# -*- coding: utf-8 -*-
"""LVADSUSR161_ia1(logistic).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YKuKI8o2A-W7PSz4bq_3eLM2LhAj2b9K
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.metrics import r2_score,mean_squared_error,accuracy_score,precision_score,recall_score,classification_report,confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/booking (1).csv')

df.head(3)

df.info()

df.describe(include='all')

df.shape

#2.5)tasks
#1)handling missing values
df.isnull().sum()

price=df['average price'].mean()
df['average price'].fillna(price,inplace=True)

room=df['room type'].mode()[0]
print(room)
df['room type']=df['room type'].fillna(room)

df['room type'].isnull().sum()

df.isnull().sum()

#checking for outliers
for columns in df.select_dtypes(include=['float64','int64']).columns:
  sns.boxplot(x=df[columns])
  plt.show()

for column in df.select_dtypes(include=['float64','int64']).columns:
  q1=df[column].quantile(0.25)
  q3=df[column].quantile(0.75)
  iqr=q3-q1
  lower_bound=q1-1.5*iqr
  upper_bound=q3+1.5*iqr
  df[column]=df[column].clip(lower=lower_bound,upper=upper_bound)
  sns.boxplot(x=df[column])
  plt.show()

#2)encoding categorical data
from sklearn.preprocessing import LabelEncoder
label=LabelEncoder()
for column in df.select_dtypes(include=['object']).columns:
  df[column]=label.fit_transform(df[column])

df.head()

#3)feature selection and data cleaning
df.duplicated().sum()
#if that so ,use below code to remove duplicates
df.drop_duplicates(inplace=True)

df.info()

#from that,we can remove some of the features
correlation=df.corr()
sns.heatmap(correlation,annot=True,fmt=".1f",cmap='coolwarm')
plt.show()

new_df=df.drop(columns=['P-C','P-not-C','number of children','number of adults','car parking space','Booking_ID','booking status','repeated'])

new_df.head()

cor=new_df.corr()
sns.heatmap(cor,annot=True,fmt=".1f",cmap='coolwarm')
plt.show()

#4)data splitting
x=df.drop(columns=['P-C'])
x=df.drop(columns=['P-C','P-not-C','number of children','number of adults','car parking space','Booking_ID','booking status','repeated'])
y=df['booking status']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=43)

#5)model development
scale=StandardScaler()
x_train_scaled=scale.fit_transform(x_train)
x_test_scaled=scale.transform(x_test)

model=LogisticRegression()
model.fit(x_train_scaled,y_train)

y_pred=model.predict(x_test_scaled)

#6)model evaluation
print("accuracy score",accuracy_score(y_test,y_pred))
print("recall:",recall_score(y_test,y_pred))
print("precision:",precision_score(y_test,y_pred))
print("F1 score:",f1_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print("confusion matrix \n",confusion_matrix(y_test,y_pred))

#while analyzing the confusion matrix, we can get the true positive as 2138
#false positive 1489 which is actual true but predicted as false
#false negative 745
#true negative 6514